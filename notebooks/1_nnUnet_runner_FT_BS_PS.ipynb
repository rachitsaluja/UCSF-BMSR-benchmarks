{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please install nnUNet v1, and run this notebook using the nnUNet conda/virtual env. \n",
    "\n",
    "### Firstly setting up environment variables\n",
    "\n",
    "Add your path here instead and copy paste in terminal in your nnUNet Environment:\n",
    "```\n",
    "export nnUNet_raw_data_base=\"/location/to/nnUNet_raw_data_base\"\n",
    "export nnUNet_preprocessed=\"/location/to/nnUNet_preprocessed\"\n",
    "export RESULTS_FOLDER=\"/location/to/nnUNet_trained_models\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Transferring of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from natsort import natsorted\n",
    "from glob2 import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnUnet_transfer_images(patients_dir, list_of_modalities, output_dir, exp_name):\n",
    "    all_patients_dir = natsorted(glob(patients_dir + '/*'))\n",
    "    \n",
    "    for i, p in enumerate(all_patients_dir):\n",
    "        for j, modality in enumerate(list_of_modalities):\n",
    "            src_file = p + '/' + p.split('/')[-1] + '_' + modality + '.nii.gz'\n",
    "            dst_file = output_dir + '/' + exp_name + '_' + '{:04}'.format(i+1) + '_000' + str(j) + '.nii.gz' \n",
    "\n",
    "            #print(dst_file)\n",
    "            os.system('cp -r '+ src_file + ' ' + dst_file)    \n",
    "            \n",
    "def nnUnet_transfer_segs(patients_dir, output_dir, exp_name):\n",
    "    \n",
    "    all_patients_dir = natsorted(glob(patients_dir + '/*'))\n",
    "    \n",
    "    for i, p in enumerate(all_patients_dir):\n",
    "        src_file = p + '/' + p.split('/')[-1] + '_seg.nii.gz'\n",
    "        dst_file = output_dir + '/' + exp_name + '_'  + '{:04}'.format(i+1) + '.nii.gz'\n",
    "\n",
    "        #print(dst_file)\n",
    "        os.system('cp -r '+ src_file + ' ' + dst_file)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Files directory\n",
    "\n",
    "Change the experiment name and path for your project/experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('mkdir /location/to/TaskXXX_UCSF_BMSR')\n",
    "os.system('mkdir /location/to/TaskXXX_UCSF_BMSR/imagesTr')\n",
    "os.system('mkdir /location/to/TaskXXX_UCSF_BMSR/imagesTs')\n",
    "os.system('mkdir /location/to/TaskXXX_UCSF_BMSR/labelsTr')\n",
    "os.system('mkdir /location/to/TaskXXX_UCSF_BMSR/labelsTs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transferring Images\n",
    "\n",
    "Change the experiment name and path for your project/experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnUnet_transfer_images(patients_dir = '/location/to/patients_2/', \n",
    "                     list_of_modalities = ['flair','t1','t2'], \n",
    "                     output_dir = '/location/to/TaskXXX_UCSF_BMSR/imagesTs', \n",
    "                     exp_name = 'TrialExp')\n",
    "\n",
    "nnUnet_transfer_images(patients_dir = '/location/to/patients_2/', \n",
    "                     list_of_modalities = ['flair','t1','t2'], \n",
    "                     output_dir = '/location/to/TaskXXX_UCSF_BMSR/imagesTs', \n",
    "                     exp_name = 'TrialExp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transferring Segs\n",
    "\n",
    "Change the experiment name and path for your project/experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnUnet_transfer_segs(patients_dir = '/location/to/patients_1/', \n",
    "                     output_dir = '/location/to/TaskXXX_UCSF_BMSR/labelsTr', \n",
    "                     exp_name = 'TrialExp')\n",
    "\n",
    "nnUnet_transfer_segs(patients_dir = '/location/to/patients_2/', \n",
    "                     output_dir = '/location/to/TaskXXX_UCSF_BMSR/labelsTs', \n",
    "                     exp_name = 'TrialExp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataset json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from batchgenerators.utilities.file_and_folder_operations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifiers_from_splitted_files(folder: str):\n",
    "    uniques = np.unique([i[:-12] for i in subfiles(folder, suffix='.nii.gz', join=False)])\n",
    "    return uniques\n",
    "\n",
    "\n",
    "def generate_dataset_json(output_file: str, imagesTr_dir: str, imagesTs_dir: str, modalities: Tuple,\n",
    "                          labels: dict, dataset_name: str, license: str = \"hands off!\", dataset_description: str = \"\",\n",
    "                          dataset_reference=\"\", dataset_release='0.0'):\n",
    "    \"\"\"\n",
    "    :param output_file: This needs to be the full path to the dataset.json you intend to write, so\n",
    "    output_file='DATASET_PATH/dataset.json' where the folder DATASET_PATH points to is the one with the\n",
    "    imagesTr and labelsTr subfolders\n",
    "    :param imagesTr_dir: path to the imagesTr folder of that dataset\n",
    "    :param imagesTs_dir: path to the imagesTs folder of that dataset. Can be None\n",
    "    :param modalities: tuple of strings with modality names. must be in the same order as the images (first entry\n",
    "    corresponds to _0000.nii.gz, etc). Example: ('T1', 'T2', 'FLAIR').\n",
    "    :param labels: dict with int->str (key->value) mapping the label IDs to label names. Note that 0 is always\n",
    "    supposed to be background! Example: {0: 'background', 1: 'edema', 2: 'enhancing tumor'}\n",
    "    :param dataset_name: The name of the dataset. Can be anything you want\n",
    "    :param license:\n",
    "    :param dataset_description:\n",
    "    :param dataset_reference: website of the dataset, if available\n",
    "    :param dataset_release:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_identifiers = get_identifiers_from_splitted_files(imagesTr_dir)\n",
    "\n",
    "    if imagesTs_dir is not None:\n",
    "        test_identifiers = get_identifiers_from_splitted_files(imagesTs_dir)\n",
    "    else:\n",
    "        test_identifiers = []\n",
    "\n",
    "    json_dict = {}\n",
    "    json_dict['name'] = dataset_name\n",
    "    json_dict['description'] = dataset_description\n",
    "    json_dict['tensorImageSize'] = \"4D\"\n",
    "    json_dict['reference'] = dataset_reference\n",
    "    json_dict['licence'] = license\n",
    "    json_dict['release'] = dataset_release\n",
    "    json_dict['modality'] = {str(i): modalities[i] for i in range(len(modalities))}\n",
    "    json_dict['labels'] = {str(i): labels[i] for i in labels.keys()}\n",
    "\n",
    "    json_dict['numTraining'] = len(train_identifiers)\n",
    "    json_dict['numTest'] = len(test_identifiers)\n",
    "    json_dict['training'] = [\n",
    "        {'image': \"./imagesTr/%s.nii.gz\" % i, \"label\": \"./labelsTr/%s.nii.gz\" % i} for i\n",
    "        in\n",
    "        train_identifiers]\n",
    "    json_dict['test'] = [\"./imagesTs/%s.nii.gz\" % i for i in test_identifiers]\n",
    "\n",
    "    if not output_file.endswith(\"dataset.json\"):\n",
    "        print(\"WARNING: output file name is not dataset.json! This may be intentional or not. You decide. \"\n",
    "              \"Proceeding anyways...\")\n",
    "    save_json(json_dict, os.path.join(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset_json(\n",
    "    output_file = \"/location/to/TaskXXX_UCSF_BMSR/dataset.json\",\n",
    "    imagesTr_dir = \"/location/to/TaskXXX_UCSF_BMSR/imagesTr\",\n",
    "    imagesTs_dir = \"/location/to/TaskXXX_UCSF_BMSR/imagesTs\",\n",
    "    modalities = ('flair', 't1', 't2'),\n",
    "    labels = {\n",
    "        \"0\": \"background\",\n",
    "        \"1\": \"FLAIR abnormal singal\"\n",
    "    },\n",
    "    dataset_name = \"Trialexp\",\n",
    "    license = \"hands off!\",\n",
    "    dataset_description = \"nnUnet experiment for Trialexp\",\n",
    "    dataset_reference = \"\",\n",
    "    dataset_release = \"1.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For Single Modality Experiments:\n",
    "'''\n",
    "generate_dataset_json(\n",
    "    output_file = \"/location/to/TaskXXX_UCSF_BMSR/dataset.json\",\n",
    "    imagesTr_dir = \"/location/to/TaskXXX_UCSF_BMSR/imagesTr\",\n",
    "    imagesTs_dir = \"/location/to/TaskXXX_UCSF_BMSR/imagesTs\",\n",
    "    modalities = ['flair'],\n",
    "    labels = {\n",
    "        \"0\": \"background\",\n",
    "        \"1\": \"FLAIR abnormal singal\"\n",
    "    },\n",
    "    dataset_name = \"Trialexp\",\n",
    "    license = \"hands off!\",\n",
    "    dataset_description = \"nnUnet experiment for Trialexp\",\n",
    "    dataset_reference = \"\",\n",
    "    dataset_release = \"1.0\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run Preprocessing\n",
    "\n",
    "\n",
    "Copy and run this on the terminal.\n",
    "\n",
    "```\n",
    "nnUNet_plan_and_preprocess -t XXX --verify_dataset_integrity\n",
    "```\n",
    "\n",
    "If everything is okay, then it will automatically run preprocessing. If not, there could be errors in your image and seg size then you need to fix it. OR if there is some small orientation differences also, it won't run automatically, here you can still do preprocessing by running the following\n",
    "\n",
    "```\n",
    "nnUNet_plan_and_preprocess -t XXX \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run training\n",
    "\n",
    "To run the training run the following on the terminal\n",
    "\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=3 nnUNet_train 3d_fullres nnUNetTrainerV2 TaskXXX_MYTASK FOLD --npz\n",
    "```\n",
    "\n",
    "FOLD = 0,1,2,3,4,all\n",
    "\n",
    "To stop training anytime, just press CTRL-C\n",
    "\n",
    "To start again\n",
    "```\n",
    "nnUNet_train 3d_fullres nnUNetTrainerV2 TaskXXX_MYTASK FOLD --npz -c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Inference\n",
    "\n",
    "To run the inference run the following on the terminal\n",
    "\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=3 nnUNet_predict -i /location/to/TaskXXX_UCSF_BMSR/imagesTs -o /location/to/TaskXXX_UCSF_BMSR/imagesTs_pred -t XXX -m 3d_fullres -f all --save_npz \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Using nnUNet\n",
    "\n",
    "Firstly have the latest version of nnUNet. \n",
    "\n",
    "`nnunet - 1.7.0`\n",
    "\n",
    "\n",
    "Idea is to preprocess the \"NEW\" data on the nnUNet plans for the model you want start fine tuning with. \n",
    "\n",
    "For example: \n",
    "\n",
    "Task111_T1_infants should be preprocessed using the plans for Task106_T1_UCSF\n",
    "\n",
    "`nnUNet_plan_and_preprocess -t XXX -pl3d ExperimentPlanner3D_v21_Pretrained -overwrite_plans /location/to/nnUNet_preprocessed/TaskXXX_XXX/nnUNetPlansv2.1_plans_3D.pkl -overwrite_plans_identifier IDENTIFIER`\n",
    "\n",
    "So for this case an example would be:\n",
    "\n",
    "```\n",
    "nnUNet_plan_and_preprocess -t 111 -pl3d ExperimentPlanner3D_v21_Pretrained -overwrite_plans /media/rnd/nnUNet_preprocessed/Task106_T1UCSF/nnUNetPlansv2.1_plans_3D.pkl -overwrite_plans_identifier TRIAL\n",
    "```\n",
    "\n",
    "\n",
    "And now that we have processed the data into the version of the model we need to fine tune. We just need to train. Which is a little different but similar:\n",
    "\n",
    "`CUDA_VISIBLE_DEVICES=0 nnUNet_train 3d_fullres nnUNetTrainerV2 TaskXXX_ExpName all -p nnUNetPlans_pretrained_IDENTIFIER --npz -pretrained_weights /location/to/model/file/3d_fullres/TaskZZZ_FineTuneModelOrigin/nnUNetTrainerV2__nnUNetPlansv2.1/all/model_final_checkpoint.model`\n",
    "\n",
    "So for this case an example would be:\n",
    "\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=0 nnUNet_train 3d_fullres nnUNetTrainerV2 Task111_T1Infants all -p nnUNetPlans_pretrained_TRIAL --npz -pretrained_weights /media/rnd/nnUNet/RESULTS_FOLDER/nnUNet/3d_fullres/Task106_T1UCSF/nnUNetTrainerV2__nnUNetPlansv2.1/all/model_final_checkpoint.model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the Batch-Size for nnUNet\n",
    "\n",
    "\n",
    "Changing the batch size is simple. Just follow the commands below, and change the name of the `pkl` file for out the output as guided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "import numpy as np\n",
    "\n",
    "### Change Task name for your task\n",
    "task_name = 'TaskXXX_UCSF_BMSR'\n",
    "preprocessing_output_dir = '/location/to/nnUNet_preprocessed'\n",
    "\n",
    "plans_fname = join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_plans_3D.pkl')\n",
    "plans = load_pickle(plans_fname)\n",
    "plans['plans_per_stage'][0]['batch_size'] = 4\n",
    "\n",
    "\n",
    "### Change name WRT to Batch Size, like I have saved it as bs4 for batch size 4\n",
    "save_pickle(plans, join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_bs4_plans_3D.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run with new plans (new batch size):\n",
    "\n",
    "Run the following command to run with the new plans for higher batch size. \n",
    "\n",
    "\n",
    "`CUDA_VISIBLE_DEVICES=0 nnUNet_train 3d_fullres nnUNetTrainerV2 TaskXXX_UCSF_BMSR all --npz -p nnUNetPlansv2.1_ps512_bs12`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the Patch-Size for nnUNet\n",
    "\n",
    "Changing the patch size is simple. Just follow the commands below, and change the name of the `pkl` file for out the output as guided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "import numpy as np\n",
    "\n",
    "### Change Task name for your task\n",
    "task_name = 'TaskXXX_UCSF_BMSR'\n",
    "preprocessing_output_dir = '/location/to/nnUNet_preprocessed'\n",
    "\n",
    "plans_fname = join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_plans_3D.pkl')\n",
    "plans = load_pickle(plans_fname)\n",
    "\n",
    "print(\n",
    "    plans['plans_per_stage'][0]['patch_size'] / np.prod(plans['plans_per_stage'][0]['pool_op_kernel_sizes'], 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the printed value is a 1X3 array where all are above 4 and they are integers. \n",
    "\n",
    "You will have to change the plans such that the patch that value IS ALWAYS:\n",
    "1. INTEGERS\n",
    "2. ABOVE 4\n",
    "3. They can all be different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans['plans_per_stage'][0]['patch_size'] = [80,160,160] ## Change patch size here\n",
    "\n",
    "## CHECK FOR CONDITION\n",
    "\n",
    "print(\n",
    "    plans['plans_per_stage'][0]['patch_size'] / np.prod(plans['plans_per_stage'][0]['pool_op_kernel_sizes'], 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change name WRT to Batch Size, like I have saved it as bs4 for batch size 4\n",
    "save_pickle(plans, join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_psNEW_plans_3D.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run with new plans (new patch size):\n",
    "\n",
    "Run the following command to run with the new plans for higher batch size. \n",
    "\n",
    "\n",
    "`CUDA_VISIBLE_DEVICES=0 nnUNet_train 3d_fullres nnUNetTrainerV2 TaskXXX_UCSF_BMSR all --npz -p nnUNetPlansv2.1_psNEW`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
